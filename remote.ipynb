{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "sliced_data_PC.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhkpK-u0xo75"
      },
      "source": [
        "function ClickConnect(){\n",
        "    console.log(\"Clicked on connect button\"); \n",
        "    document.querySelector(\"colab-connect-button\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)"
      ],
      "id": "VhkpK-u0xo75"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdeeb981"
      },
      "source": [
        "### Import packages"
      ],
      "id": "fdeeb981"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzwS02XfR7C7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "833698dd-b349-433b-d7ac-95156a799afb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "xzwS02XfR7C7",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr4Kcdn936rz"
      },
      "source": [
        "# ! tar tf /content/drive/MyDrive/frames/frames.tar | awk -F/ '{if (NF<6) print }'"
      ],
      "id": "sr4Kcdn936rz",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Dgc4onMOxJ"
      },
      "source": [
        "# ! tar --delete -f /content/drive/MyDrive/frames/frames.tar 'data/content/'"
      ],
      "id": "P3Dgc4onMOxJ",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPuS4rA7SPGB"
      },
      "source": [
        "# ! tar -cvzf /content/drive/MyDrive/hmdb_tar /content/drive/MyDrive/hmdb51_org/"
      ],
      "id": "HPuS4rA7SPGB",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4efqnmZCMWkQ"
      },
      "source": [
        "# ! cp -v /content/drive/MyDrive/hmdb_tar /teste.tar"
      ],
      "id": "4efqnmZCMWkQ",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4WDsjRxMpXS"
      },
      "source": [
        "# ! rm /teste"
      ],
      "id": "L4WDsjRxMpXS",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38ef1b20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e827e8-0689-4904-e63f-b373e0538ddc"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import cv2\n",
        "import os\n",
        "from os import path\n",
        "import json\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from enum import Enum\n",
        "from datetime import date\n",
        "\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import random\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "#if gpu, use it\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "id": "38ef1b20",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e2b5948"
      },
      "source": [
        "### Extract frames from videos and place them inside video name folder"
      ],
      "id": "9e2b5948"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88f3e612"
      },
      "source": [
        "#  def extract_frames(file_path, target_dir):\n",
        "#     if not os.path.exists(target_dir):\n",
        "#         os.makedirs(target_dir)\n",
        "\n",
        "#     vidcap = cv2.VideoCapture(file_path)\n",
        "#     success, image = vidcap.read()\n",
        "#     count = 0\n",
        " \n",
        "#     while success:\n",
        "#         frame_path = os.path.join(target_dir, f'{count}.jpg')\n",
        "#         cv2.imwrite(frame_path, image)     # save frame as JPEG file\n",
        "#         count += 1\n",
        "#         success, image = vidcap.read()\n",
        "   \n",
        "\n",
        "# root = '/data/content/drive/MyDrive/hmdb51_org'\n",
        "# frame_root = '/data/result/frames'\n",
        "# labels_path = '/data/results/labels.csv'\n",
        "# class_name_to_label_path = '/data/results/class_name_to_label.json'\n",
        "\n",
        "# #read files\n",
        "# files = []\n",
        "\n",
        "# for class_name in os.listdir(root):\n",
        "#     for video_name in os.listdir(os.path.join(root, class_name)):\n",
        "#          files.append([os.path.join(class_name, video_name), class_name])\n",
        "\n",
        "# #normalize labels\n",
        "# class_name_to_label = {}\n",
        "# current_label = -1\n",
        "\n",
        "# for vid in files:\n",
        "#     label = class_name_to_label.get(vid[1], -1)\n",
        "\n",
        "#     if label == -1:\n",
        "#         current_label += 1\n",
        "#         class_name_to_label[vid[1]] = current_label\n",
        "#         label = current_label\n",
        "\n",
        "#     vid[1] = label\n",
        "\n",
        "\n",
        "# # save file paths\n",
        "# if not os.path.exists(os.path.split(labels_path)[0]):\n",
        "#      os.makedirs(os.path.split(labels_path)[0])\n",
        "\n",
        "# f = open(labels_path, 'w')\n",
        "\n",
        "# f.write('path,label\\n')\n",
        "\n",
        "# for vid in files:\n",
        "#     f.write(f'{vid[0]},{vid[1]}\\n')\n",
        "\n",
        "# f.close()\n",
        "\n",
        "#  # save label normalization\n",
        "# if not os.path.exists(os.path.split(class_name_to_label_path)[0]):\n",
        "#     os.makedirs(os.path.split(class_name_to_label_path)[0])\n",
        "\n",
        "# with open(class_name_to_label_path, 'w') as json_file:\n",
        "#     json.dump(class_name_to_label, json_file, indent=4)\n",
        "\n",
        "# # extract frames\n",
        "# for i, vid in enumerate(files):\n",
        "#     file_path = os.path.join(root, vid[0])\n",
        "#     target_dir = os.path.join(frame_root, vid[0])\n",
        "\n",
        "#     extract_frames(file_path, target_dir)\n",
        "\n",
        "#     print(f'{i+1}/{len(files)}')"
      ],
      "id": "88f3e612",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8313adf"
      },
      "source": [
        "### Split labels csv file into train and validate - 80/20"
      ],
      "id": "f8313adf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "59fda1df"
      },
      "source": [
        "#  import pandas as pd\n",
        "\n",
        "# def split(label, path):\n",
        "#     train = label.sample(frac=0.8, random_state=201)\n",
        "#     val = label.drop(train.index)\n",
        "#     train.to_csv(path + '/train.csv', mode='a', header=False)\n",
        "#     val.to_csv(path + '/val.csv', mode='a', header=False)\n",
        "\n",
        "\n",
        "# root = '/data/result/frames'\n",
        "# frame_root = '/data/result/frames'\n",
        "# labels_path = '/data/results/labels.csv'\n",
        "# class_name_to_label_path = '/data/results/class_name_to_label.json'\n",
        "\n",
        "# labels_data = pd.read_csv(labels_path)\n",
        "# last_label = labels_data.tail(1)['label']\n",
        "# last_label = last_label.iloc[0]\n",
        "# max_labels = labels_data.nunique()\n",
        "# max_labels = max_labels['label']\n",
        "# print(max_labels)\n",
        "# index = 0\n",
        "\n",
        "# for i in range(max_labels):\n",
        "#     label = labels_data[labels_data['label'] == index]\n",
        "#     split(label, root)\n",
        "#     index +=1"
      ],
      "id": "59fda1df",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1W_0UTaTAhq"
      },
      "source": [
        "# ! tar -cvf /content/drive/MyDrive/frames/frames.tar /data"
      ],
      "id": "q1W_0UTaTAhq",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZIFoKy4RxWu",
        "outputId": "0af03ca2-f777-4c8e-f2ae-eaa11388417b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! mkdir /data"
      ],
      "id": "4ZIFoKy4RxWu",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/data’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBnqXFbDs8SC"
      },
      "source": [
        "! tar -xvf /content/drive/MyDrive/frames/frames.tar -C /data"
      ],
      "id": "BBnqXFbDs8SC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a01d179"
      },
      "source": [
        "### custom dataset"
      ],
      "id": "4a01d179"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f476ebcf"
      },
      "source": [
        "\n",
        "train_csv_path = '/data/data/result/frames/train.csv'\n",
        "validate_csv_path = '/data/data/result/frames/val.csv'\n",
        "root_path = '/data/data/result/frames/'\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, frames_csv_file, root_dir):\n",
        "        self.frames_csv = pd.read_csv(frames_csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.slice_size = 10\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frames_csv)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "      \n",
        "      #need to take 10 frames and stack them on 1 tensor                                                    \n",
        "      #()-> second index has to be 1 because id column (3 columns) \n",
        "      #()-> why have to be NOT??? is wrong!! but works..\n",
        "        all_frames = [f for f in os.listdir(os.path.join(self.root_dir, str(self.frames_csv.iloc[idx, 1]))) if not os.path.isfile(f)]\n",
        "      #random get 10 consecutive frames from all video frames in folder\n",
        "        start = random.randrange(len(all_frames) - self.slice_size)\n",
        "        frames = all_frames[start: start + self.slice_size]\n",
        "        all_frames.sort()\n",
        "        images = []\n",
        "      #load the images\n",
        "        for frame in frames:\n",
        "            path = os.path.join(self.root_dir, str(self.frames_csv.iloc[idx, 1])) + '/' + frame\n",
        "            images.append(Image.open(path).convert('RGB'))\n",
        "        transform = transforms.Compose([\n",
        "                    T.Resize((140, 140),),\n",
        "                    T.ToTensor(),\n",
        "                    T.Normalize(mean = [0.43216, 0.394666, 0.37645], std = [0.22803, 0.22145, 0.216989])])\n",
        "        #apply tansforms and condense all the images in 1 tensor\n",
        "        tensors = []\n",
        "        for image in images:\n",
        "            tensors.append(transform(image))            \n",
        "        final_image = torch.stack(tensors, dim=1)\n",
        "        tag = int(self.frames_csv.iloc[idx, 2])\n",
        "        return final_image, tag\n",
        "\n",
        "train_dataset = MyDataset(train_csv_path, root_path)\n",
        "validate_dataset = MyDataset(validate_csv_path, root_path)\n"
      ],
      "id": "f476ebcf",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtmNWZEPx3GN"
      },
      "source": [
        "## dataloaders"
      ],
      "id": "XtmNWZEPx3GN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cd1207d"
      },
      "source": [
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
        "validate_dataloader = DataLoader(validate_dataset, batch_size=6, shuffle=True)"
      ],
      "id": "8cd1207d",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abovlZX1x5-V"
      },
      "source": [
        "## training loop"
      ],
      "id": "abovlZX1x5-V"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "215f13dd"
      },
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, stats):\n",
        "    size = len(dataloader.dataset)\n",
        "    count = 0\n",
        "    for X,y in dataloader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        stats['train_loss'].append(loss.item())\n",
        "        if count % 100 == 0:\n",
        "            loss = loss.item()\n",
        "            print(f\"loss: {loss:>7f}\")\n",
        "        count += 1\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, stats):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    stats['val_loss'].append(test_loss)\n",
        "    stats['val_acc'].append(correct)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "id": "215f13dd",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQzk6B_ix9Ft"
      },
      "source": [
        "## prepare the model"
      ],
      "id": "VQzk6B_ix9Ft"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d089ba0a"
      },
      "source": [
        "class_name_to_label_path = '/data/data/results/class_name_to_label.json'\n",
        "model_path = '/content/drive/MyDrive/saved/model.pt'\n",
        "f = open(class_name_to_label_path)\n",
        "classes = json.load(f)\n",
        "f.close()\n",
        "\n",
        "model_ft = torchvision.models.video.r3d_18(pretrained=True, progress=True)\n",
        "for param in model_ft.parameters():\n",
        "    param.requires_grad = False\n",
        "model_ft.fc = nn.Linear(512, len(classes))\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = torch.optim.Adam(model_ft.parameters(), lr=2e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "  checkpoint = torch.load(model_path)\n",
        "  model_ft.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer_ft.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  current_epoch = checkpoint['epoch']\n",
        "  loss_ft = checkpoint['loss']\n",
        "  model_ft = model_ft.to(device)\n",
        "  num_tries = str(date.today())\n",
        "else:\n",
        "  current_epoch = 0\n",
        "num_epochs = 35\n",
        "# # Decay LR by a factor of 0.1 every 7 epochs\n",
        "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.1)"
      ],
      "id": "d089ba0a",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebdb15d8"
      },
      "source": [
        "## train and evaluate"
      ],
      "id": "ebdb15d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4m05Jn7wWzs"
      },
      "source": [
        "# ! rm /content/drive/MyDrive/saved/model.pt"
      ],
      "id": "e4m05Jn7wWzs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd535171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "c2527015-1842-46e8-ea5d-dc10f7aeb57a"
      },
      "source": [
        "stats = {'train_loss':[], 'val_loss':[], 'val_acc':[]}\n",
        "for t in range(current_epoch, num_epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model_ft, loss_fn, optimizer_ft, stats)\n",
        "    test_loop(validate_dataloader, model_ft, loss_fn, stats)\n",
        "    json_stats = json.dumps(stats)\n",
        "    f = open('/content/drive/MyDrive/saved/' + num_tries + '.json', 'w')\n",
        "    f.write(json_tries)\n",
        "    f.close()\n",
        "    torch.save({\n",
        "            'epoch': t+1,\n",
        "            'model_state_dict': model_ft.state_dict(),\n",
        "            'optimizer_state_dict': optimizer_ft.state_dict(),\n",
        "            'loss': loss_ft,\n",
        "            }, model_path)\n",
        "\n",
        "print(\"Done!\")\n"
      ],
      "id": "cd535171",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 3.040680\n",
            "loss: 2.758498\n",
            "loss: 3.162730\n",
            "loss: 2.522709\n",
            "loss: 2.738160\n",
            "loss: 2.869355\n",
            "loss: 3.207178\n",
            "loss: 3.216038\n",
            "Test Error: \n",
            " Accuracy: 31.1%, Avg loss: 2.802671 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-4b5eaf3afc42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mjson_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/saved/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_tries\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_tries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     torch.save({\n",
            "\u001b[0;31mNameError\u001b[0m: name 'json_tries' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Psj7atDhyA_t"
      },
      "source": [
        "## Show results graphs"
      ],
      "id": "Psj7atDhyA_t"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03eba39c"
      },
      "source": [
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "x = np.arange(0, 10, 0.1)\n",
        "y = stats['train_loss']\n",
        "z = stats['val_loss']\n",
        "ax.plot(y, color='blue', label='train_loss')\n",
        "ax.plot(z, color='grey', label='val_loss')\n",
        "ax.set_xlabel('epochs')\n",
        "ax.set_ylabel('loss value')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "x = np.arange(0, 10, 0.1)\n",
        "z = stats['val_loss']\n",
        "ax.plot(z, color='grey', label='val_loss')\n",
        "ax.set_xlabel('epochs')\n",
        "ax.set_ylabel('loss value')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "z = stats['val_acc']\n",
        "ax.plot(z, color='grey', label='val_acc')\n",
        "ax.set_xlabel('epochs')\n",
        "ax.set_ylabel('acc value')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ],
      "id": "03eba39c",
      "execution_count": null,
      "outputs": []
    }
  ]
}